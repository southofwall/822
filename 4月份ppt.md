[TOC]

3.28-4.3

​	  计划：学习ppt，阅读周志华的《机器学习》，了解相关算法思想

 	遇到的问题：矩阵、概率、特殊算法函数、梯度...稍复杂计算推导看不懂	

​	 解决方法：先放着...用上了再学

4.4-4.11	

​	  计划：下载配置mindspore，熟悉基本操作，按照ppt完成基本操作

​	  实际：简单了解mindspore，需要python相关知识，暂时放着。ppt看完了，

4.12-4.17

​      计划：看完吴恩达课程，有多余时间就学一下python

本周感想：多项式问题、决策问题->复杂函数拟合的可行性？

​					神经网络算法是算法的堆砌？

4.18-4.24

计划：吴恩达深度学习课程，python

本周感想：算法的本身即是一种生产力，或许会成为新时代的一种思考方式

4.25-5.9

计划：吴恩达深度学习课程，python



### 3.31

##### 模型评估和选择

1.模型的评估和选择

评估方法：留出法、交叉验证法、自助法

调参？最终模型？

性能尺度：错误率和精度，查全率和查准率和F1和Fβ（涉及调和平均和加权调和平均）

ROC（真正例率）和AUC（auRc ）：ROC表示混淆矩阵，AUC求积ROC

代价敏感错误率和代价曲线： 不同的错误有不同的代价



2.比较检验

假设检验： 假设，验证正确

t检验：多次重复留出或交叉检验法，处理多个测试错误率

交叉验证t检验：针对两个不同的学习器

McNemar（卡方检验）：检测两学习器差异性

Friedman检验：检测多个学习器差异

和Nemenyi后续检验：具体哪两个学习器存在差异

泛化误差：偏差（期望预期和真实结果的偏差）

方差（训练集变动导致学习性能的差异）

噪声（期望泛化误差下界）



### 4.1

##### 线性回归

线性回归：最小二乘法，多元线性回归，对数线性回归，广义线性模型（处理y）

对数几率回归：Sigmoid函数，极大似然法

线性判别分析（LDA）：**最大化类间均值，最小化类内方差**。意思就是将数据投影在低维度上，

​	并且投影后同种类别数据的投影点尽可能的接近，不同类别数据的投影点的中心点尽可能的远。 

多分类学习：OVO OVR MVM

类别不平衡问题：过采样，欠采样，决策”再缩放“->”阈值移动“

np问题：非确定性多项式难题（大一统理论，没有固定解法，但是可检验）

p类问题：有一定解法，可检验   np难：难以解决，难以验证



### 4.2

##### 决策树

决策树：二分类问题

信息熵：度量样本集合纯度

信息增益：元素对结果的贡献

增益率：

基尼指数：验证划分后错误率

预剪枝和后剪枝：是否继续划分or划分后删除

连续值处理：离散化找划分点

缺失值处理：求信息熵，再求子集增益，权重调整

多变量决策树：



##### 神经网络

神经元模型：阈值10->Sigmoid函数

感知机：由输入输出层构成，容易处理线性可分的问题（与或非，异或用两层）

多层前馈神经网络：与下层完全连接，隐层中蕴含连接权和阈值

误差逆传播（BP）算法：反馈调整，容易过拟合，提前终止法

全局最小和局部最小：梯度下降，模拟退火，遗传算法

RBF，ART，SOM，级联，Elman，BoltMann机



##### 支持向量机SVM

超平面，支持向量：超平面将数据划分开，“支持”超平面的最近几个样本点

对偶问题：KKT条件，SMP算法

核函数：处理映射高维问题

软间隔支持向量机：允许有不满足约束点，损失函数，正则化约束风险

支持向量回归、核方法



##### 贝叶斯分类器

贝叶斯决策论：使条件风险最小，要利用贝叶斯定理获得后验概率

极大似然估计：先假定具有某种确定的概率分布形式，再基于训练样本进行参数估值

朴素贝叶斯：满足独立性

半朴素贝叶斯：引进独赖性估计

贝叶斯网：借助有向无环图来刻画关系，条件概率表来描述属性联合分布

EM算法（与Kmean算法相似）：估计参数隐变量

​			E步：给定一个近似初始值，推测变量分布

​			M步：根据E步得分布反向推导初始值，最大化概率



##### 集成学习

基学习器：同质（决策树，神经网络）

组件学习器：不同质

强依赖：Boosting 调整样本分布对基学习器赋权或重采样，不满足条件丢弃

可并行： Bagging 样本采样得多组样本，训练不同基学习器

随机森林：在Bagging的基础上，引入了随机属性选择（子集）

结合优势：提高泛化能力，避免陷入局部绩效点，扩大假设范围

结合策略：平均法（差异大加权），投票法（过半，相对，加权）

  				学习法（初样本预测数据训练次级样本）

误差-分歧分解and多样性度量

多样性增强：数据样本扰动（采样法），输出属性（取子集），输出（翻转，子任务）

​                       调参改约束

##### 聚类

无标记训练样本解释内在性质和规律

性能度量：外部指标（与参考模型比较）内部指标（直接考察内部结果，距离指标）

距离计算：闵min可夫斯基距离，VDM

原型聚类：K均值算法，取均值向量不断聚集成簇

学习向量量化（LVQ）：近点学习靠近，与k-mean都是向量模型

高斯聚合混类：概率模型

密度聚类：核心对象作为种子，找同类，被异类包围

层次聚类：不断融合最近的簇

##### 降维与度量学习

k近邻学习：找靠近的k个训练样本，根据样本信息继续预测下去

低维嵌入：高维下样本稀疏，距离计算困难，产生“维数灾难”

多维缩放（MDS）：要求空间中的样本点距离在低维空间得到保持

主成分分析（PCA）：投影到一条线上，n成分就n维度

 		最近重构性（样本点到超平面距离够近），最大可分性（投影尽可能分开）

核化线性降维：非线性映射

流形学习：拓扑流型概念降维

​	等度量映射（Isomap）：高维直接计算两点距离不恰当，应计算测地线距离

​	局部线性嵌入（LLE）：局部保持线性关系

度量学习：将高维样本的点的分布规律学习下来放到低维上

##### 特征选择

特征子集搜索 + 子集评价机制 -> 特征选择方法

a 过滤式：特征选择过滤数据集，再训练学习器

b 包裹式：学习器的性能评价特征子集

c 嵌入式和L1正则化：特征选择和学习器一起训练

稀疏表示与字典学习：提高数据稀疏程度有利于数据学习

压缩感知：利用原本的稀疏性恢复原本的数据

##### 计算学习理论

研究通过”计算“来”学习“的理论，分析学习任务的困难本质，为学习算法提供理论保证

概率近似正确（PCA）：较大概率学得误差满足预设上限的模型

有限假设情况：可分情况，不可分情况

VC维：无限假设空间，假设VC维

Rademacher复杂度：在VC维基础上考虑数据分布

稳定性：研究输入变化时输出变化情况

##### 半监督学习

影响模型性能较大的因素打上标签

主动学习：全部打标签生成模型

纯半监督学习和直推学习：前者预测待测数据，后者预测未标签数据

生成式方法：假设模型，基于EM算法进行极大似然估计求解缺失参数代入

半监督SVM：不考虑未标记划分超平面，考虑后找到既能划分又能找到低密度区域的

图半监督学习：数据集映射成图，强关联生成边，标记染色

基于分歧的方法：具有差异的学习器通过协同训练，提高泛化能力，减小模型假设、数据规模影响

半监督聚类：必不连或必相连，少量标记可直接作为种子

### 4.3

##### 概率图模型

隐马尔科夫模型：状态函数，转移函数

马尔科夫随机场：网状，势函数（因子），生成式模型，处理联合概率

条件随机场：判别式模型，处理的是条件概率

学习和推断：变量消去，信念传播（处理边际分布问题）

近似推断：MCMC采样（采集分布），变分推断（大分类再细分）

话题模型：在文档内取词构成话题

##### 规则学习

数理逻辑的刻画表达

存在“冲突“：集成被判定结果不同的多条规则覆盖，可用投票法，排序法，元规则法（最小子集）

序贯覆盖：利用分治的算法逐个清空训练样例，自上而下or自下而上，剪枝优化

一阶规则学习：比较取代逻辑定义，适用于引入领域知识 

归纳逻辑程序设计：离散最小一般泛化（合并同类项），逆归结（拆分）

##### 强化学习

探索-利用窘境：马尔可夫决策四元组<X，A，P，R>

​		E-贪心：按照一定概率探索然后选择平均奖赏最高的

​		softmax：按照平均奖赏来决定选取的概率

有模型学习：转移概率P已知，奖赏R已知，设状态空间X和动作空间A有限

​	策略评估和改进：评估策略期望累积奖赏并执行，最大化累计奖赏并设定约束条件

​	策略迭代：不断评估改进寻求最优解，动态规划

免模型学习：转移奖赏机制未知

​	蒙特卡罗学习：多次采样取均值获得期望

​	时序差分学习：增量式更新，解决蒙特卡洛的批处理问题

值函数近似：状态空间连续，状态无穷多，设计一个函数尽可能地接近状态

模仿学习：从范例中学习

​	直接模范学习：学习专家“状态-动作对”

​	逆强化学习：奖赏函数困难，我们从结果反推

### 3.28（卷积神经网络）

1.划分

**监督学习**：学习过程调整，有正确结果

**无监督学习**：数据处理，输出依赖数字特征

**半监督学习**：部分标签化分类

**强化学习**：评价学习，增强学习，强化正确输出

**泛化**：模型适用于新样本的能力



2.常见算法

a.**贝叶斯**

b **决策树**：非参数10，有监督

c **正则化**：解决拟合问题

d **基于核的算法**：支持向量机（**SVM**）、基于核的Fisher判别分 析（KFD）、核主成分分析（KPCA）等。

e **关联学习**：BFS广度优先搜索 ( broad first search)和DFS 深度优先搜索 ( depth first search) 两类。

f **回归学习**

g **聚类学习**：无监督学习

h **集成算法**：多模型混用

过拟合问题

1. 数据噪声太大
2. 特征太多
3. 模型太复杂

过拟合的解决办法：

1. 清洗数据
2. 减少模型参数，降低模型复杂度
3. 增加惩罚因子（正则化），保留所有的特征，但是减少参数的大小（magnitude）。



3.简单神经网络

MP模型：阈值触发

赫布规则：突触可塑性

感知机：线性分类，二维到三维

非线性神经元：激励函数，解决非线性问题

代价函数（损失函数，目标函数）：自动调整神经网络的权重和偏置

梯度下降：不断调整代价函数使误差值减小

反向传播BP：输出向输入流动 



4.卷积神经网络

卷积：方块乘积

池化层：采平均值或极值

全连接层：记录各个结点面

弃权、提前终止：避免过拟合

交叉熵代价函数：学习缓慢，允许有更大的误差

批标准化BN：解决ICS问题，加快训练收敛虚度，防止梯度爆炸或消失，防止过拟合

内部变量转移（ICS）：数据从输入层往后层传递时，各层输出的分布与对应的

​                                     输入信号的分布不同，即每层的数据分布会变。

梯度下降（SGD）、随机梯度下降、小批量随机梯度下降：<img src="C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230329003142937.png" alt="image-20230329003142937" style="zoom: 25%;" />

动量法



名词

数据集：事件或对象表现或性质，通过特定算法学习得模型。可分为训练集、测试样本

归纳和演绎：普适性到特例

NFL定理表明**没有一个学习算法可以在任何领域总是产生最准确的学习器**（机器学习的局限性）

发展历程：连接主义、符号主义、结构学，泛型和方法，样例学习（决策树） 

​                    ILP（归纳逻辑程序设计）

​                   统计学习（支持向量机SVM），神经网络深度学习（连接主义）



### 3.29（循环神经网络）



1.循环神经网络(Recurrent Neural Network，RNN)主要用于处理**序列数据**，其最大的特点就是神经元在**某时刻**的输出可以作为输入再次输入 到神经元，这种串联的网络结构非常适合于**时间序列数据**



2.长短期记忆（LSTM）

为解决长期依赖、梯度消失和梯度爆炸的问题

输入输出遗忘

记忆细胞状态和隐藏状态  短时和长时切换



3.基于RNN的语言模型

a 词典向量化

b 概率输出运算 建模

c 监督学习，标签训练

d 交叉熵误差优化



### 3.30（深度学习模型优化）

1.参数初始化

无理论，优化和泛化之间的矛盾，初始化原则具有“破坏相似性”

权重初始化方法：启发式方法，随机正交矩阵，稀疏初始化



2.超参数寻优算法（学习率、模型深度等）

手动搜索：调整模型的有效容量以匹配任务的复杂性

自动超参数寻优：网格搜索和随机搜索



3基于梯度的自适应学习方法

基础算法：参数在每个维度都使用同一个学习率

AdaGrad算法：不同学习率η分别应用于各个维度的参数，会过度收缩

基于梯度的自适应学习算法（RMSProp）：小批量随机梯度平方项进行加权平均

基于梯度的自适应学习算法（AdaDelta）：引入状态变量

Adam算法：较优



4.生成对抗神经网络（GAN）

定义一个模型作为生成器（Generator），能够输入一个**向量**， 输出手写数字大小的像素**图像**。

定义一个分类器作为判别器（Discriminator）用来判别图片是 真的（来自数据集）还是假的（生成器中生成的），输入为 手写**图片**，输出为判别图片的**标签**。



5.迁移学习

基于预训练模型(一个场景训练好的模型)迁移到正在处理的特定问题，自动化地提取更具表现力的特 征，以及满足了实际应用中的端到端 (End-to-End) 需求。

可行性：

6.强化学习

强化学习的经典算法(sarsa算法)



7.模型优化的局限性

a 局部最小值问题

b 梯度消失：浅层学习慢于深层、梯度爆炸：不收敛、梯度悬崖：跳过最优解

c 长期依赖：变深的结构使模型丧失回到学习先前信息的能力

d 非精确性：有偏估计小批量数据估计，目标函数不可分用近似梯度

### 4.4

**张量(tensor)**：多维数组，创造更高维度的矩阵（类比向量，一组数在基坐标上的不同表示）

**算子：**关系的映射，类比函数

**Cell**：Cell是MindSpore核心编程结构，定义了执行计算的基本模块（类对象？）

LeNet-5：有一篇论文，先去学习一下如何阅读论文...

### 4.5

##### Lenet-5（abstract）

摘要：使用反向传播算法训练的多层神经网络，基于梯度技术在最少的预处理条件合成一个复杂的决策曲面

​			对手写体的高维模式进行分类，CNN就是为了处理二维形状可变性设计的。

​			文档识别系统：字段提取、分割、识别、和语言建模

​			提出GTN：允许使用基于梯度的方法对这样的多模块系统进行全局训练，最小化整体性能指标

​			OCR（光学字符识别）：扫描成像，对比文字库（特征描述算法），匹配文字

数据处理pipeline

​	加载 - shuffle（清洗、打乱）- map（比对）- batch（分析处理）-repeat

​	zip（合并）

模型定义

​	卷积（convolution）：

​	激活函数：

​	全连接（Dense）：

​	网络定义:

​	Loss：

​	优化器（Optimizer）：

### 4.6（目标检测和识别，卷积神经网络的应用）

传统图像处理+机器学习算法

​	步骤：滑动窗口定位，提取视觉特征，分类器分类 

​	不足：选择区域不佳，特征不具有适应性

> ​	针对滑动窗口问题，提出了候选区域的解决方案，先预测所有存在再分类
>
> ​	滑动窗口类似于暴力搜索，而候选区域则是基于先验的启发式搜索

 基于深度学习的目标检测方法

​	图像的特征提取和基于神经网络的目标识别和定位，卷积神经网络（CNN）

​	基于**区域建议**和**基于回归**的目标检测与识别算法

R-CNN算法

​	输入图像，候选框缩放，归一化提取CNN特征

​    SVM分类识别，线性回归微调，每个类别单独训练一个[边框回归器](https://www.cnblogs.com/wangguchang qing/p/10393934.html)

​	三大问题：重复计算、多阶段训练、所需空间大、耗时长

SPP（空间金字塔池化）

​	候选区域：裁剪目标不全、缩放目标形变

​	<img src="C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230405200852943.png" alt="image-20230405200852943" style="zoom:50%;" />



conbolution layers：卷积层

spatial pyramid pooling layer：空间金字塔池化层

fixed-length representation：固定长度表示

​	R-CNN是直接从原始图片中提取特征，它在每张原始图片上提取2000个Region Proposal，然后对	每一个候选区域框进行一次卷积计算，差不多要重复2000次，而SPP-net则是在卷积原始图像之后	的特征图上提取候选 区域的特征。所有的卷积计算只进行了一次，效率大大提高

​	**缺点：**和R-CNN一样，它的训练要经过多个阶段，特征也要存在磁盘中，另外，SPP中的微调只更新	spp层后面的全连接层，而对于一个新的任务，有必要对卷积层也进行 微调。

Fast R-CNN算法

​	特征提取：整张图片为输入利用CNN得到图片的特征层

​	region proposal：通过Selective Search等方法从原始图片提取区域候选框，并把这些候选框一一	投影到最后的特征层；

​     区域归一化：针对特征层上的每个区域候选框进行RoI Pooling操作，得 到固定大小的特征表示；        	分类与回归：然后再通过两个全连接层，分别用softmax做多目标分类。 

![image-20230405232940518](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230405232940518.png)

### 4.7

**Faster R-CNN**

1.滑动窗口降低维度

2.滑动窗口位置，RPN都基于k个固定比例的anchor box生成anchors

3.每个rigion proposal由 ①objectness分数 ②4个表征改区域边界框的坐标

选出128个positive anchors和128个negativeanchors训练

**RPN网络**（**Region Proposal Networks**）

网络提取更少的质量更高的窗口， 来进一步改善目标检测的速度和精度，并有较高的召回率

类别得分：softmax分类positive和negative

边框回归：计算anchors对应的bounding box 的平移和缩放量 （regression偏移量），以获得精确的					Reg proposal

proposal层：负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时						剔除太小和超出边界的proposals

**proposal层**

生成anchors排序选择提取修正后的位置，超出的positive anchors为图像边界

剔除尺寸小的，对剩余的positive anchors进行NMS（非极大值抑制）

![image-20230407232916253](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230407232916253.png)

**训练RPN**

**非极大值抑制（NMS）**

置信度得分排序，置信度最高的单独拿出来

计算每个边框的IoU=交集/并集，删除IoU大于阈值的边界框（多张脸）

<img src="C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230407234454445.png" alt="image-20230407234454445" style="zoom: 80%;" />

### 4.8

**基于回归预测的目标检测算法**

其主要思路是均匀的在图片的不同位置进行密集的抽样， 类似前面提高的候选框，该抽样采用的是不同尺度的长宽比，然后利用CNN提取特征后进行直接分类和回归，整个过程只需要一步，在检测速率上有明 显提高。

**SSD算法**

one-stag目标检测算法，所谓的one-stag目标检测算法， 是指不需要产生候选区域，通过一个神经网络就可以直接输出目标的位置和类别，而SSD算法与Faster RCNN最大区别在于采用多尺度特征图预测

SSD的锚框是动态变换的，特征图越大，所产生的锚框必定越小。SSD不适用于小目标的检测

**YOLO算法**

（分网格，目标出现在网格中，网格负责继续预测）

Yolo算法直接将原始图片分割成互不重合的小方块，然后通过卷积操作获得2×2大小 的特征图像，特征图像上的每个元素对应原始图片的一个小方格，用这个元素可以预测 那些中心点在该小方格内的目标

7×7（网格数）×30（每个网格对应的预测结果向量的维数）

box 需要求（x,y,w,h,c）五个量，中心点，缩放比例，置信度

**损失函数**

累积各项平方差

### 4.9（循环神经网络RNN）

图像描述：（图像分类，目标检测，图像描述）

模板方法，检索方法（依赖数据集）

**编码-解码结构方法（注意力机制优化）**

该结构解决输入序列与输出序列长度不一致而无法直接使用RNN的问题

（输入编码器CNN编码 -> 特征编码 -> 解码器输出序列 LSTM词嵌入）端到端，监督学习，CNN+RNN

**后输入覆盖问题（注意力机制）**

根据时间步设置不同权值输出，让模型在生成不同的单词时关注图像中对应区域的特征

1）空间注意力（区域）

划分区域，时间步的长短期记忆单元隐藏状态和图像区域注意模型决定权值，区域特征指导单词

2）高层语意注意力（语义）

多标签分类器提取图像中存在的物体名称属性，并将这些属性向量送入注意力机制（特定信息）

3）通道注意力（通道）

Fsq：特征压缩，具有全局感受野的池化操作，Fex：参数w为特征通道生成权值

**典型词嵌入方法（Word2evc）**

Skip-Gram方法：中心词计算上下文

似然函数、损失函数

对照词汇表确定context的one-hot向量oc，结合嵌入矩阵E，生成嵌入向量ec，然后输入softmax分类器，得到所有可能目标词在这里出现的概率p(t|c)，θt为与输出t有关的参数。y为target的one-hot向量

**MSCOCO**网站

### 4.10

安装pycharm

### 4.11（梯度下降batch）

 Octave  Matlab

损失函数

![image-20230410195242582](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230410195242582.png)

![image-20230410195308312](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230410195308312.png)

三维梯度图->二维等高线

![image-20230410200806750](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230410200806750.png)

![image-20230410202839208](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230410202839208.png)

$\Theta_1:=\Theta_1-\alpha\frac{d}{d\Theta_1}J(\Theta_1)$

学习率： $ \alpha $

梯度：根据导数不断移动$\frac{d}{d\Theta_1}J(\Theta_1)$

Batch梯度下降：each step of gradient descent uses all the training examples 



### 4.12（向量化、特征矢量化）

![image-20230411131401095](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230411131401095.png)

变量矢量化（并行运算节省时间）

<img src="C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230411132347363.png" alt="image-20230411132347363" style="zoom:50%;" />

![image-20230411133417026](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230411133417026.png)

**特征缩放**

特征的量的大小相差过大时，我们可以进行特征缩放$ x_i={x_i-\mu\over\lambda} $ 

根据图像迭代曲线变化判断梯度收敛，学习率$ \alpha $的设置

特征工程和多项式回归的多功能性

### 4.13（逻辑回归，正则化）

逻辑回归：处理分类问题，线性回归拟合问题

![img](https://pic1.zhimg.com/v2-f8ba4d8fa17e81c5010da63312e88b7c_r.jpg)

sigmoid函数：$g(x)=1\over1+e^(-x)$

决策边界：我们可以用非常复杂的模型来适应非常复杂形状的判定边界

逻辑回归中的代价函数

解决过拟合：更多的数据，特征子集，正则化减小参数大小

![image-20230412133317422](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230412133317422.png)

![image-20230412133621085](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230412133621085.png)

简化公式

![image-20230412133847575](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230412133847575.png)

实现梯度下降

![image-20230412215937498](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230412215937498.png)

正则化：鼓励更小的w，使图像曲线更为光滑

![image-20230412222925204](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230412222925204.png)

### 4.14（神经网络）

**应用**

![image-20230413235302653](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230413235302653.png)

**内部逻辑**

![image-20230414000439289](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230414000439289.png)

增添阈值决定值是否继续传递

![image-20230414001237432](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230414001237432.png)

这里的话理解一下上标的2和3即可

### 4.15  4.16

#### 3.1-3：代码结构，先写每层再写model

4：前向传播的实现

6：矩阵运算

三种常见的激活函数

![image-20230414210559054](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230414210559054.png)

多分类问题

![image-20230415231339335](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230415231339335.png)

**softmax**

![image-20230415152332687](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230415152332687.png)

#### Adam（梯度下降优化4.1）

![image-20230415234629296](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230415234629296.png)

通过比较损失函数评估算法（交叉验证测试集）

![image-20230416112607825](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230416112607825.png)

通过偏差与方法进行诊断

![image-20230416132935367](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230416132935367.png)

bias误差：函数和测试集的差距

variance方差：测试集和数据集的差距

![image-20230416162544563](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230416162544563.png)

学习曲线

![image-20230416160843715](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230416160843715.png)

![image-20230416161139829](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230416161139829.png)

variance（高方差，过拟合）bias（高偏差，拟合函数出现了问题）

![image-20230416161734900](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230416161734900.png)

大型神经网络从理论上满足：大数据，多运算

开发迭代

![image-20230416165638340](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230416165638340.png)

误差分析

数据生成（提高鲁棒性）

![image-20230416165742288](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230416165742288.png)

3.5简单讲了一下机器学习项目  4.0.1讲了查全率和查准率，可以看书

### 4.17（决策树）

**人工**寻找**分支特征**不断**提高纯度**

停止决策：决策过深or决策完毕or约束

纯度和熵

![image-20230417132950725](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230417132950725.png)

数据越偏，熵越小，纯度越大，下面是熵函数H（p）的计算

![image-20230417133041347](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230417133041347.png)

信息增益

![image-20230417143350364](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230417143350364.png)

![image-20230417143409018](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230417143409018.png)

对于数值如何划分？根据信息增益拆分

多决策树：投票法

抽样法——>随机森林：样本抽样，部分特征，多个决策树

XGBoost（注意力专注重于未做好的子集，刻意练习）

决策树VS神经网络：决策树仅仅在小样本训练速度速度快和可解释性上略胜一筹，在分类问题有优势

 神经网络：可迁移性，关联性，数据类型广度，图像，文本信息处理问题

### 4.18（聚类）

**K-means**：初始化N个点，迭代距离训练找下一个中心点

![image-20230418093940374](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230418093940374.png)

随机初始化点，根据成本函数选择最优解

![image-20230418094529434](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230418094529434.png)

选取聚类数量K：肘部法则or现实问题具体分析

**异常检测算法**，高斯分布，开发和评估

![image-20230418141616186](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230418141616186.png)

异常检测算法主动发现异常情况，K-means将例子归纳到所属特征

### 4.19（基于内容or基于协同过滤推荐系统）

线性回归学习，成本函数

![image-20230419092158085](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230419092158085.png)

线性回归和协同过滤算法，给二元标签加Sigmoid函数

![image-20230419093300080](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230419093300080.png)

均值归一化（结果归0）

![image-20230419100525165](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230419100525165.png)

基于内容or基于协同过滤推荐系统

### 4.20（强化学习）

（状态，行动，奖励，下一状态）

![image-20230421110956359](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230421110956359.png)

状态转移误差的假设（随机强化学习）

![image-20230421115503339](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230421115503339.png)

神经网络改进：不局限一个值，四个值一起练

贪婪算法改进：95%最优，5%随机

### 4.21 简单回顾所学知识

### 4.22调整一天

 钢琴考试、三下乡面试，周六补课

### 4.23（神经网络）

标准神经网络，卷积神经网络，循环神经网络

![image-20230423140222211](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230423140222211.png)

![image-20230423140304188](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230423140304188.png)

geoffrey hinton

胶囊算法、良好的心态

### 4.24

神经网络训练流程：二元分类，逻辑回归，损失函数，梯度下降，导数、

链式法则求导计算：1->1.001,6->6.003

### 4.28-5.8（准备数模校赛）



异常检测：单点（达到某个界定值），上下文（特定场景下出现异常），集体异常（集体情况发生改变）

难点：1.无标签，监督学习不适用   **2.需要区别噪声和异常点，比较抽象**

方法：无监督，专家认证

时序相关和时序独立：取决于是否依赖时间

![image-20230430154112670](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230430154112670.png)

![image-20230430154147649](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230430154147649.png)

全局检测和局部检测

![image-20230430154849925](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230430154849925.png)

![image-20230430154925376](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230430154925376.png)

![image-20230430155155899](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230430155155899.png)

![image-20230430155208112](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230430155208112.png)

![image-20230430155248604](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230430155248604.png)

**fraudar**

![image-20230502091246052](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230502091246052.png)

SliceNDice

![image-20230502093452129](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230502093452129.png)

deepFD

![image-20230502093542613](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230502093542613.png)

![image-20230502155130151](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230502155130151.png)

DAGMM（降维）

![image-20230502160257240](C:\Users\honor\AppData\Roaming\Typora\typora-user-images\image-20230502160257240.png)

MSCRED（时间序列，有点像）

而到了真正写论文的三天，我们遇到了以下问题：

**虚拟区域是什么？**

一开始我以为是检测一条水管，将中间分成了8个区域，求和之后发现这些值围绕0上下波动，

以为就是用多元线性回归方程建立，y（求和值）与x（各个区域流量差值）

**流量差值数据表中数据？**

我们理解过程如下：这个差值是否应该围绕0上下波动，一开始在围绕3上下波动的情况

是否说明已经发生泄露（题目表述也不太清楚，我们就认为这是不同流量计的特性差异了）；

**异常值的定义与后面的对检测异常结果解释？**

我们给数据按照标准打上异常标签，又用划定异常的标准制定了异常检测模型，然后还要说明异常

检测结果的合理性，"过拟合"是无法避免的，我们认为我们小组读不懂题目，建立异常检测标准和

检测异常似乎是同一件事情，我们最后也是做的稀里糊涂的。

**模型优化结果不明显？**

我们采取了很多我们理解范围内的很多方法，正态分布，箱线图，求梯度的五点求导法......最后我们发现

无论我们采用那种方法，无非围绕平均值、方差标准差、斜率这些基本的要素，认知和方法储备还是远远不够。

有时一些看似合理的优化还有着背道而驰的作用。

**回顾**

我们真正建模时间满打满算也就三天，期间还要上课，其实真正坐在电脑前的估计也就一天半，我们没有实现什么，

只是接受网上繁杂的相关信息，将其杂糅输出实现了一遍。花费时间最多的其实是在写论文过程上，找到思路，

将方法理解实现，再用有逻辑一点的语言表述出来，再向其中掺杂一点自己不切实际的思考，便交出了我们的答卷......

